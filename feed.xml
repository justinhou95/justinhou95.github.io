<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://justinhou95.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://justinhou95.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-22T11:44:57+00:00</updated><id>https://justinhou95.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">When Marčenko-Pastur meets Bures-Wasserstein</title><link href="https://justinhou95.github.io/blog/2025/rmt/" rel="alternate" type="text/html" title="When Marčenko-Pastur meets Bures-Wasserstein"/><published>2025-10-01T15:12:00+00:00</published><updated>2025-10-01T15:12:00+00:00</updated><id>https://justinhou95.github.io/blog/2025/rmt</id><content type="html" xml:base="https://justinhou95.github.io/blog/2025/rmt/"><![CDATA[<h3 id="wishart-matrix">Wishart matrix</h3> <p>The study of empirical covariance matrices dates back to the work of John Wishart [1]. Let \((x^{(n)})_{n\in\mathbb{N}}\) be i.i.d. samples from an underlying distribution, the empirical covariance matrices is given by</p> \[\Sigma_n = \frac{1}{n}X_n X_n^\top, \quad X_n = \begin{bmatrix} x^{(1)} &amp; \dots &amp; x^{(n)}\\ \end{bmatrix}.\] <p>For simplicity, let us always consider the underlying distribution to be the standard Gaussian \(\mathcal N(0,I)\). Then by our favorite law of large number, \(\Sigma_n\) converges to \(I\).</p> <h3 id="marčenko-pastur-distribution">Marčenko-Pastur distribution</h3> <p>This above convergence surprisingly fails in the high-dimension setting when \(d = n\) (or more generally \(d/n \to \lambda\) but we take \(\lambda = 1\) for simplicity). Marčenko and Pastur proved a limiting spectral distribution in their seminal paper [2]. Let us consider the empirical spectral distribution</p> \[\eta_n = \frac{1}{n}\sum_{i=1}^{n}\delta_{\lambda_n}\] <p>where \((\lambda_i)_{i=1}^n\) are eigenvalues of \(\Sigma_n\). Then \(\eta_n\) converges weakly to \(\mathrm{MP}\), known as the Marčenko-Pastur distribution. In our simple case, the Marčenko-Pastur distribution \(\mathrm{MP}\) has density</p> \[f(x) = \frac{1}{2\pi x} \sqrt{(4-x)x}\] <p>supported on \([0,4]\) (see density of general Marčenko-Pastur distribution \(\mathrm{MP}(\lambda, \sigma^2)\) on <a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Wiki</a>). This is not only mathematically beautiful but also demonstrate why empirical spectral could be misleading in high-dimension regime.</p> <h3 id="bures-wasserstein">Bures-Wasserstein</h3> <p>Let’s set the beautiful random matrix theory aside for now and view \(\Sigma_n\) in the space of positive semidefinite matrix \(\mathcal{S}^n_{+}\). For any \(A, B \in \mathcal{S}^n_{+}\), one may consider \(\mu\sim \mathcal N(A)\) and \(\nu \sim \mathcal N(B)\) be centered Gaussian distributions with covariances \(A\) and \(B\) respectively. Recall that the Wasserstein-2 distance between any two distributions \(\mu, \nu \in \mathcal P(\mathbb R^d)\) is defined by</p> \[\mathcal W_2^2(\mu,\nu) = \inf_{X\sim \mu, Y\sim \nu} \mathbb E[\Vert X - Y\Vert^2].\] <p>The quadratic cost aligns naturally with the covariance structure of Gaussian measures, which leads to a closed-form expression for the Wasserstein-2 distance between Gaussian distributions given by</p> \[\mathcal W_2^2(\mu, \nu) = \mathrm{tr}(A + B - 2(A^{1/2}BA^{1/2})^{1/2}).\] <p>Since every centered Gaussian distribution is uniquely determined by its covariance matrix, \(\mathcal W_2\) on Gaussians induce a distance on \(\mathcal{S}^n_{+}\) such that \(d_{\mathrm{BW}}(A,B) = \mathcal W_2(\mu,\nu)\), and the distance is known as the Bures-Wasserstein distance. It turns out that \(\mathcal{S}^n_{+}\) equipped with the \(d_{\mathrm{BW}}\) carries a natural Riemannian manifold structure. We refer the interested reader to [3] for a detailed account of this geometry. For now, we go back to our Wishart matrices.</p> <h3 id="bures-wasserstein-distance-of-wishart-matrix">Bures-Wasserstein distance of Wishart matrix</h3> <p>Now, we can view \(\Sigma_n\) in the space \((\mathcal{S}^n_{+}, d_{\mathrm{BW}})\). It is naturally to ask what is the Bures-Wasserstein distance between \(\Sigma_n\) and \(I\)? Does it also converges to some number as \(n\) goes to infinity? Note that by the close form above, we have</p> \[\frac{1}{n}d^2_{\mathrm{BW}}(\Sigma_n, I) = \frac{1}{n}\mathrm{tr}(\Sigma_n + I - 2\Sigma_n^{1/2})\] <p>So, we only need to know the limit spectral of \(\Sigma_n\), which is already given by the Marčenko-Pastur distribution above. After some calculation:</p> \[\frac{1}{n}\mathrm{tr}(\Sigma_n + I) \to \mathbb E_{\Lambda\sim \mathrm{MP}}[\Lambda] + 1 = 2,\] \[\frac{1}{n}\mathrm{tr}(\Sigma_n^{1/2}) \to \mathbb E_{\Lambda\sim \mathrm{MP}}[\sqrt{\Lambda}] = \int_0^4 \sqrt{x}\frac{1}{2\pi x} \sqrt{(4-x)x} \, dx = \frac{8}{3\pi},\] <p>we obtain that</p> \[\lim_{n \to \infty} \frac{1}{n}d^2_{\mathrm{BW}}(\Sigma_n, I) = \lim_{n \to \infty} \frac{1}{n}\mathrm{tr}(\Sigma_n + I - 2\Sigma_n^{1/2}) = 2(1-\frac{8}{3\pi})\] <p>Finally, we test it numerically and witness the desired convergence.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt_1-480.webp 480w,/assets/img/rmt_1-800.webp 800w,/assets/img/rmt_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Hope you also find this result cute! I am currently working on this, so if you are interested in this, please feel free to reach out and I would be delighted to discuss.</p> <hr/> <h3 id="reference">Reference</h3> <p>[1] Wishart, John. “The generalised product moment distribution in samples from a normal multivariate population.” Biometrika 20.1/2 (1928): 32-52.</p> <p>[2] Marčenko, Vladimir A., and Leonid Andreevich Pastur. “Distribution of eigenvalues for some sets of random matrices.” Mathematics of the USSR-Sbornik 1.4 (1967): 457.</p> <p>[3] Takatsu, Asuka. “On Wasserstein geometry of Gaussian measures.” Probabilistic approach to geometry 57 (2010): 463-472.</p> <p>[4] <a href="https://djalil.chafai.net/blog/2010/04/30/wasserstein-distance-between-two-gaussians/">Wasserstein distance between two Gaussians</a></p> <p>[5] <a href="https://djalil.chafai.net/blog/2011/01/29/the-marchenko-pastur-law/">The Marchenko-Pastur law</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Wishart matrix]]></summary></entry><entry><title type="html">When causal and classical Wasserstein are equivalent</title><link href="https://justinhou95.github.io/blog/2025/equivalence/" rel="alternate" type="text/html" title="When causal and classical Wasserstein are equivalent"/><published>2025-10-01T15:12:00+00:00</published><updated>2025-10-01T15:12:00+00:00</updated><id>https://justinhou95.github.io/blog/2025/equivalence</id><content type="html" xml:base="https://justinhou95.github.io/blog/2025/equivalence/"><![CDATA[<p><strong>Notation</strong> We denote by \(\mathbf{S} \subset \mathbb{R}^{T\times T}\) the set of symmetric matrices, \(\mathbf{S}_+ \subset \mathbb{R}^{T\times T}\) the set of symmetric positive semi-definite matrices, \(\mathbf{L}\subset \mathbb{R}^{T\times T}\) the set of lower triangular matrices, \(\mathbf{L}_+\subset \mathbb{R}^{T\times T}\) the set of lower triangular matrices with non-negative diagonal, \(\mathbf{D}\subset \mathbb{R}^{T\times T}\) the set of diagonal matrices, \(\mathbf{O}\subset \mathbb{R}^{T\times T}\) the set of orthogonal matrices. We denote by \(\|\cdot \|_2\) the operator norm and \(\|\cdot\|_{\mathrm{F}}\) the Frobenius norm. The condition number of a matrix \(A\) is \(\kappa_2(A) = \| A^{-1}\|_2 \|A\|_2\).</p> <hr/> <h3 id="adapted-wasserstein-distance">Adapted Wasserstein distance</h3> <p>The adapted Wasserstein distance is introduced as a strengthend version of classical Wasserstein distance by incorporating the temporal causal structure in order to guarantee robustness of stochastics optimization problem. Recall classcial Wasserstein distance \(\mathcal{W}_2\) is defined via</p> \[\mathcal{W}_2^2(\mu,\nu) = \inf_{(X,Y) \sim (\mu,\nu)} \mathbf{E}[|X - Y|^2],\] <p>while the adapted Wasserstein distance \(\mathcal{AW}_2\) (with nested representation) is defined via</p> \[\mathcal{AW}_2(\mu,\nu) = \inf_{(X,Y) \sim_{\rm bc} (\mu,\nu)} \mathbf{E}^{1/2}[|X - Y|^2],\] <p>where \(\sim_{\rm bc}\) imposes addition bicausal restriction on \((X,Y) \sim (\mu,\nu)\), so trivially \(\mathcal{W}_2\leq \mathcal{AW}_2\). This bicausal restriction enforces non-anticipativity in both directions and thus provide a natural and robust framework for stochastic optimization problems i.e. for many optimal values of stochastic optimization problem \(\mathcal{V}(\mu)\) we have</p> \[|\mathcal{V}(\mu) - \mathcal{V}(\nu)| \lesssim \mathcal{AW}_2(\mu,\nu)\] <p>but this does not hold for \(\mathcal{W}_2\).</p> <h3 id="gaussian-processes">Gaussian processes</h3> <p>For Gaussian processes, both \(\mathcal{W}_2\) and \(\mathcal{AW}_2\) have closed-form solutions. For simplicity we consider centered real-valued Gaussian process.</p> <p>Then \(\mathcal{W}_2\) is given by</p> \[\mathcal{W}_2^2(\mu,\nu) = \min_{O\in\mathbf{O}}\Vert A^{1/2} - B^{1/2}O\Vert_2^2 =: d_{\rm BW}^2(A,B),\] <p>and \(\mathcal{AW}_2\) is given by</p> \[\mathcal{AW}_2^2(\mu,\nu) = \min_{D\in\mathbf{D}\cap \mathbf{O}}\Vert L - MO \Vert_2^2 =: d_{\rm ABW}^2(L,M),\] <p>where \(A,B\) are covairance matrices of \(\mu,\nu\) and \(A = LL^\top\), \(B = MM^\top\) are Cholesky decomposition with non-negative diagonals.</p> <h3 id="when-causal-and-non-causal-distances-are-equivalent">When causal and non-causal distances are equivalent?</h3> <p>It is not difficult to construct examples when \(\mathcal{W}_2(\mu,\nu) \to 0\) while \(\mathcal{AW}_2(\mu,\nu)\) is large, so \(\mathcal{W}_2\) and \(\mathcal{AW}_2\) can not be equivalent in general (they induce different topology). However, in Gaussian case, it turns out that in a ball around regular Gaussian process, they are actually equivalent i.e. there exists \(C &gt; 0\) such that for all \(\mu, \nu\) in the ball</p> \[\mathcal{W}_2(\mu,\nu) \leq \mathcal{AW}_2(\mu,\nu) \leq C\cdot \mathcal{W}_2(\mu,\nu).\] <p>Let \(A \in \mathbf{S}_+\), and \(LL^\top = A\) be the Cholesky decomposition where \(L \in \mathbf{L}_+\). For a small perturbation \(\Delta A \in \mathbf{S}\), let \((L+\Delta L)(L + \Delta L)^\top = A + \Delta A\) be the Cholesky decomposition where \(\Delta L \in \mathbf{L}\). By the stability result of Cholesky decomposition; see [1], we know that if \(\|\Delta A\|_{\mathrm{F}} &lt; \frac{1}{2\| A^{-1}\|_2}\), then the Cholesky decomposition of \(A+\Delta A\) is unique and</p> \[\|\Delta L\|_F \leq (2 + \sqrt{2})\| A^{-1}\|_2\| A\|_2^{1/2}\|\Delta A\|_{\mathrm{F}}. \tag{1}\] <p>Now, denote \(B = A + \Delta A\) and \(M = L + \Delta L\). Note that for all \(O \in \mathbf{O}\), we have</p> \[\begin{aligned} \|A - B\|_\mathrm{F} &amp;\leq \| A^{\frac{1}{2}} A^{\frac{1}{2}} - A^{\frac{1}{2}}OB^{\frac{1}{2}} + A^{\frac{1}{2}}OB^{\frac{1}{2}} - B^{\frac{1}{2}}B^{\frac{1}{2}}\|_2 \\ &amp;\leq \| A^{\frac{1}{2}} \|_2 \| A^{\frac{1}{2}} - OB^{\frac{1}{2}}\|_\mathrm{F} + \| O B^{\frac{1}{2}} \|_2 \|A^{\frac{1}{2}} - B^{\frac{1}{2}} O^\top\|_\mathrm{F}\\ &amp; \leq (\| A^{\frac{1}{2}} \|_2 + \| B^{\frac{1}{2}} \|_2) \|A^{\frac{1}{2}} - B^{\frac{1}{2}} O^\top\|_\mathrm{F}. \end{aligned}\] <p>Taking the minimum over all \(O \in \mathbf{O}\) yields that</p> \[\|\Delta A\|_\mathrm{F} \leq (\| A^\frac{1}{2}\|_2 + \| B^\frac{1}{2}\|_2) d_{\rm BW}(A,B).\] <p>Plugging this estimate to (1), we obtain</p> \[d_{\rm ABW}(L,M) \leq (2 + \sqrt{2})\| A^{-1}\|_2\| A\|_2^{1/2}\| (\| A^\frac{1}{2}\|_2 + \| B^\frac{1}{2}\|_2) d_{\rm BW}(A,B).\] <p>Let \((A_t)_{t\in [0,1]}\) be a geodesic on \((\mathbf{S}_{+},d_{\rm BW})\) connecting \(A\) to \(B\), then</p> \[d_{\rm ABW}(L,M) \leq \left(\int_0^1 2(2 + \sqrt{2})\| A_t^{-1}\|_2\| A_t\|_2^{1/2}\| \| A_t^\frac{1}{2}\|_2 dt\right)d_{\rm BW}(A,B).\] <p>Therefore, on a geodesically convex set where \(\| A^{-1}\|_2\| A\|_2^{1/2}\| \| A^\frac{1}{2}\|_2\) upper-bounded, the following equivalence holds:</p> \[d_{\rm ABW}(L,M) \leq C\cdot d_{\rm BW}(A,B).\] <h3 id="discussion">Discussion</h3> <p>The take home message is: if you’re working on distributional robustness around a regular Gaussian processes, causal and non-causal distances are equivalent, so just pick one you like.</p> <p>A natural next question is whether this could be extended to more general cases. Extending to multi-dimensional Gaussian processes case I believe is quite manageable. Extending this to general ``regular” stochastic processes is a bit more subtle. To me, there are at least two challenging questions to answer. First, what is the right characterization of regularity? Second, how can one estimate distances without the privilege closed-form solutions? If you are also interested in this and have an idea, feel free to reach me out. If you worry that this would make the study of causal distance useless, also reach me out and I am also happy to chat about that.</p> <hr/> <h3 id="reference">Reference</h3> <p>[1] X-W Chang and Damien Stehl´e. “Rigorous perturbation bounds of some matrix factorizations”. SIAM journal on matrix analysis and applications 31.5 (2010), pp. 2841–2859.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Notation We denote by \(\mathbf{S} \subset \mathbb{R}^{T\times T}\) the set of symmetric matrices, \(\mathbf{S}_+ \subset \mathbb{R}^{T\times T}\) the set of symmetric positive semi-definite matrices, \(\mathbf{L}\subset \mathbb{R}^{T\times T}\) the set of lower triangular matrices, \(\mathbf{L}_+\subset \mathbb{R}^{T\times T}\) the set of lower triangular matrices with non-negative diagonal, \(\mathbf{D}\subset \mathbb{R}^{T\times T}\) the set of diagonal matrices, \(\mathbf{O}\subset \mathbb{R}^{T\times T}\) the set of orthogonal matrices. We denote by \(\|\cdot \|_2\) the operator norm and \(\|\cdot\|_{\mathrm{F}}\) the Frobenius norm. The condition number of a matrix \(A\) is \(\kappa_2(A) = \| A^{-1}\|_2 \|A\|_2\).]]></summary></entry><entry><title type="html">Gaussian coupling is Gaussian?</title><link href="https://justinhou95.github.io/blog/2025/gaussian/" rel="alternate" type="text/html" title="Gaussian coupling is Gaussian?"/><published>2025-01-18T15:12:00+00:00</published><updated>2025-01-18T15:12:00+00:00</updated><id>https://justinhou95.github.io/blog/2025/gaussian</id><content type="html" xml:base="https://justinhou95.github.io/blog/2025/gaussian/"><![CDATA[<blockquote class="block-warning"> <p>If \(X,Y\) are Gaussian random variables. Is \((X,Y)\) Gaussian?</p> </blockquote> <p>If you google it, there are many counter-examples proving why this is NOT true in general. However, I find much less discussion on whether this could be true under some condition. So I decide to share my thoughts and also a proof from a optimal transport point of view.</p> <hr/> <h3 id="counter-example">Counter example</h3> <p>We start with a well-known counter-example:</p> <p>Let \(X\) be a standard Gaussian, \(W\) be coin-flip i.e. a Bernoulli random variable taking values 1 and −1 with equal probability. If we define \(Y = W X\), then \(Y\) is still standard Gaussian by symmetric argument. However \(X+Y\) is not even Gaussian because \(X+Y = 0\) holds with probability \(1/2\).</p> <p>Another observation is that \(\mathbb{E}[XY] = 0\). So this is also an example when uncorrelated but dependent. Anyhow we have a counter example.</p> <h3 id="another-simple-example">Another simple example</h3> <p>Let \(X,Y\) standard Gaussian with correlation \(1\) i.e. \(\mathbb{E}[XY] = 1\). Then \(\mathbb{E}[(X-Y)^2] = 0\), so \(X=Y\). Therefore \((X,Y)\) is Gaussian!</p> <p>It is not difficult to generalize this to multi-dimension. Let \(X,Y\) be standard Gaussian on \(\mathbb{R}^d\) and assume \((X,Y)\) has covariance matrix: \(\begin{bmatrix}I&amp;P\\ P^\top&amp;I \end{bmatrix}.\) If \(P\) is orthogonal <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup>, i.e. \(PP^\top = I\), then similarly we get</p> \[\mathbb{E}[(X-PY)^2] = \mathrm{tr}\Big(\begin{bmatrix}I &amp; -P\end{bmatrix}\begin{bmatrix}I&amp;P\\ P^\top&amp;I \end{bmatrix}\begin{bmatrix}I \\ -P^\top\end{bmatrix}\Big) = 0.\] <p>So \(Y = P^\top X\) and \((X,Y)\) is Gaussian.</p> <h3 id="source-of-randomness">Source of randomness</h3> <p>The source of randomness is the key. The covariance structure is able to restrict the source of randomness. If the correlation is so high, there is no room for external randomness to enter. Then the only source of randomness is the marginal, which in our case a Gaussian randomness. As a result, the whole distribution is a linear transformation of the marginal, hence Gaussian.</p> <p>Our simple example assume marginal to be standard Gaussian, never the less, the proof generalize to general non-degenerate Gaussian marginals, by transforming marginals back to standard Gaussian.</p> <h3 id="non-degenerate-marginals">Non-degenerate marginals</h3> <p>We consider \(X \sim \mathcal{N}(a,A)\) and \(Y \sim \mathcal{N}(a,B)\) where \(A,B\) are positive semidefinite matrix. We represent the covariance matrix of \((X,Y)\) by \(\begin{bmatrix}A&amp;C\\ C^\top&amp;B \end{bmatrix}.\) Then by using the Cholesky decomposition \(A = LL^\top\) and \(B= MM^\top\), we transform \((X,Y)\) to \((L^{-1}X, M^{-1}Y)\) and simplify the problem to our simple example before:</p> \[\mathrm{Cov}\Big(\begin{bmatrix}L^{-1}X\\ M^{-1}Y\end{bmatrix}\Big) = \begin{bmatrix}I&amp;L^{-1}C(M^{-1})^{\top}\\ M^{-1}C^\top (L^{-1})^{\top}&amp;I \end{bmatrix}.\] <p>Therefore, if \(C = LPM^{\top}\) where \(P\) is orthogonal, \((X,Y)\) is Gaussian.</p> <blockquote class="block-tip"> <h4 id="theorem-1">Theorem 1</h4> <p>Let \(X \sim \mathcal{N}(a,A)\) and \(Y \sim \mathcal{N}(a,B)\) non-degenerate Gaussians. If \((X,Y)\) has covariance matrix of form: \(\begin{bmatrix}A&amp;C\\ C^\top&amp;B \end{bmatrix}.\) where \(C = LPM^{\top}\) and \(P\) is orthogonal, then \((X,Y)\) is Gaussian</p> </blockquote> <h3 id="optimal-transport-point-of-view">Optimal transport point of view</h3> <p>You might think how this related to optimal transport. If you have a hammer everything looks like a nail. Optimal transport is a huge topic with deep connections to many fields of mathematics. Here we only need the basic form:</p> \[\min_{X\sim \mu, Y\sim \nu}\mathbb{E}[|X-Y|^2].\] <p>Intuitively, it is finding the best way to couple \(X,Y\) given \(X\sim \mu\) and \(Y\sim \nu\). One of the most celebrated result in optimal transport is the Brenier theorem.</p> <blockquote class="block-tip"> <h4 id="breniers-theorem">Brenier’s theorem</h4> <p>Suppose \(\mu,\nu\) have second moments and \(\mu\) is absolutely continuous with respect to the Lebesgue measure. Then there exists a unique optimal coupling.</p> </blockquote> <p>Actually Brenier’s theorem is much more than this, but this is already enough for us now. In our setting, \(\mu\) and \(\nu\) are non-degenerate Gaussian, so the setting of Brenier’s theorem is fulfilled. By the existence part of Brenier’s theorem, there exists \((X,Y)\) which obtains the minimum. Again we denote the covariance matrix of \((X,Y)\) by \(\begin{bmatrix}A&amp;C\\ C^\top&amp;B \end{bmatrix}.\)</p> <p>Notice that the cost functional \(\mathbb{E}[|X-Y|^2]\) only depends on the mean and covariance of \((X,Y)\). So any \((X,Y)\) with the right mean and covariance would obtain the minimum. Of course, a Gaussian coupling with mean \(\begin{bmatrix}a \\ b \end{bmatrix}\) and covariance \(\begin{bmatrix}A&amp;C\\ C^\top&amp;B \end{bmatrix}\) is an optimal coupling.</p> <p>By the uniqueness part of Brenier’s theorem, there is only one optimal coupling. So there is a unique distribution of \((X,Y)\) with with mean \(\begin{bmatrix}a \\ b \end{bmatrix}\) and covariance \(\begin{bmatrix}A&amp;C\\ C^\top&amp;B \end{bmatrix}\) subjected to \(X\sim\mu\) and \(Y\sim\nu\), which is the Gaussian one above.</p> <p>From the discussion, we see if the covariance matrix of the coupling is a optimal coupling, then the coupling is Gaussian. In general, one may consider optimal transport problem with more general quadratic cost:</p> \[\min_{X\sim \mu, Y\sim \nu}\mathbb{E}[|X-\Lambda Y|^2].\] <p>If \(\Lambda\) is invertible, Brenier’s theorm still applies by a change of variable. Therefore, similarly one can prove Theorem 1 from an optimal transport approach, <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup>.</p> <h3 id="further-discussion">Further discussion</h3> <p>In this blog, we are in the setting where marginal are non-degenerate. It would also be interesting to see how this generalizes when marginals are degenerate.</p> <h4 id="references">References</h4> <ul> <li><a href="https://link.springer.com/book/10.1007/978-3-540-71050-9">Optimal Transport: Old and New</a></li> <li><a href="https://arxiv.org/abs/2412.18794">Entropic adapted Wasserstein distance on Gaussians</a></li> </ul> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:2" role="doc-endnote"> <p>\(P\) is a contraction matrix if and only if \(\begin{bmatrix}I&amp;P\\ P^\top&amp;I \end{bmatrix}\) is positive semidefinite. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1" role="doc-endnote"> <p>One can also prove through the closed form <a href="https://djalil.chafai.net/blog/2010/04/30/wasserstein-distance-between-two-gaussians/">Wasserstein distance between two Gaussians</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[If \(X,Y\) are Gaussian random variables. Is \((X,Y)\) Gaussian?]]></summary></entry><entry><title type="html">Accidentally commit large files or sensitive data</title><link href="https://justinhou95.github.io/blog/2024/commit-large-files/" rel="alternate" type="text/html" title="Accidentally commit large files or sensitive data"/><published>2024-11-03T15:12:00+00:00</published><updated>2024-11-03T15:12:00+00:00</updated><id>https://justinhou95.github.io/blog/2024/commit-large-files</id><content type="html" xml:base="https://justinhou95.github.io/blog/2024/commit-large-files/"><![CDATA[<h3 id="what-if-i-accidentally-commit-something">What if I accidentally commit something？</h3> <p>Commit in Git is just like taking a snapshot of some of your data and save it in a folder called <code>.git</code>. Imagine you take snapshots of your room everyday. But one day you realize a sticky note with your bank password is on the wall! Although, you can tear off or avoid including it in the snapshots in the future. But anyone with your snapshot history will see it! In a nutshell, what you add/modify/delete in the working directory and what you commit in the future will not affect your commit history. They are just quietly lying there.</p> <h3 id="where-are-them">Where are them?</h3> <p>Sometimes, you even don’t know where is your large files. So it is good to check them first.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git rev-list --objects --all |
  git cat-file --batch-check='%(objecttype) %(objectname) %(objectsize) %(rest)' |
  sed -n 's/^blob //p' |
  sort --numeric-sort --key=2 |
  cut -c 1-12,41- |
</code></pre></div></div> <p>This will give you human-readable output like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
0d99bb931299  530KiB path/to/some-image.jpg
2ba44098e28f   12MiB path/to/hires-image.png
bd1741ddce0d   63MiB path/to/some-video-1080p.mp4
</code></pre></div></div> <ul> <li><a href="https://stackoverflow.com/questions/10622179/how-to-find-identify-large-commits-in-git-history">https://stackoverflow.com/questions/10622179/how-to-find-identify-large-commits-in-git-history</a></li> </ul> <h3 id="rewrite-history">Rewrite history</h3> <p>Changing/deleting the history is highly risky, so make sure you really need to do it! There are also other ways you can share your repo without sharing the history or sharing part of the history. If you really think certain files should be deleted in the history in your repo, carefully follows tutorials:</p> <ul> <li><a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/removing-sensitive-data-from-a-repository">https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/removing-sensitive-data-from-a-repository</a></li> <li><a href="https://www.geeksforgeeks.org/how-to-remove-a-large-file-from-commit-history-in-git/">https://www.geeksforgeeks.org/how-to-remove-a-large-file-from-commit-history-in-git/</a></li> <li><a href="https://rtyley.github.io/bfg-repo-cleaner/">https://rtyley.github.io/bfg-repo-cleaner/</a></li> </ul> <p>At the end, don’t forget to force push to the server because you have already rewritten the history.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git push --force
</code></pre></div></div> <h3 id="good-practice">Good practice</h3> <p>Next time, add the sticker note with your password into <code>.gitignore</code> so that it will not be included in the snapshot.</p> <ul> <li><a href="https://git-scm.com/docs/gitignore">https://git-scm.com/docs/gitignore</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[What if I accidentally commit something？]]></summary></entry><entry><title type="html">Best friends building Python package</title><link href="https://justinhou95.github.io/blog/2024/build-project/" rel="alternate" type="text/html" title="Best friends building Python package"/><published>2024-08-03T15:12:00+00:00</published><updated>2024-08-03T15:12:00+00:00</updated><id>https://justinhou95.github.io/blog/2024/build-project</id><content type="html" xml:base="https://justinhou95.github.io/blog/2024/build-project/"><![CDATA[<p>I’ve recently published a Python package. At least for me, I become more careful and comprehensive to the codes, once decided publishing the project. Immediately, I stand on the opposite an start questioning and criticizing every piece of codes. But I have to say this helps me improve the codes a lot. For example, in machine learning projects, I start thinking how flexible my structure could be adapted to other data and model. Publishing your codes if definitely something you want to try!</p> <blockquote> <p>“If I have seen further it is by standing on the shoulders of giants.” <br/> –Issac Newton</p> </blockquote> <p>There are fantastic already fantastic tools and pipelines which helps you build a robust, flexible, clean and beautiful project. In this post, I would like to introduce those friends of mine to you.</p> <h3 id="venv">venv</h3> <p>Python environments can drive one crazy. There are lots of nice tools but personally I prefer <code>venv</code> which is a standard library of Python. First you create a virtual enviroment with</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>python3.9 <span class="nt">-m</span> venv .venv
<span class="gp">$</span><span class="w"> </span><span class="nb">source</span> .venv/bin/activate
</code></pre></div></div> <p>This creates a .venv folder to store your environment. You can gitignore it by adding <code>.venv/</code> to <code>.gitignore</code>. Now you can start managing your packages.</p> <ul> <li><a href="https://snarky.ca/how-virtual-environments-work/">How virtual environments work</a></li> <li><a href="https://github.com/github/gitignore/blob/main/Python.gitignore">gitignore template</a></li> </ul> <h3 id="pip--pip-tools">pip + pip-tools</h3> <p>Pip is also a standard library of Python which helps you manage your dependency. My practice is to use with pip-tools.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip setuptools pip-tools
</code></pre></div></div> <p>First you need a <code>.in</code> specifying the packages you need, e.g.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># requirements/train.in
torch 
seaborn
...
</code></pre></div></div> <p>Based on this file, piptools find a list of packages you need. You can output them in a <code>.txt</code> file.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>python <span class="nt">-m</span> piptools compile requirements/train.in <span class="nt">--output-file</span> requirements/train.txt
</code></pre></div></div> <p>Now you have all the packages you need in the <code>.txt</code> file, e.g.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#
# This file is autogenerated by pip-compile with Python 3.9
# by the following command:
#
#    pip-compile --output-file=requirements/train.txt requirements/train.in
#
absl-py==2.1.0
    # via ml-collections
annotated-types==0.6.0
    # via pydantic
anyio==4.3.0
...
</code></pre></div></div> <p>At the end, we just need to install these packages from pip:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">-r</span> requirements/train.txt
</code></pre></div></div> <p>If you also integrate your package into environment in an <a href="https://setuptools.pypa.io/en/latest/userguide/development_mode.html">editable mode</a>, run</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>python <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--editable</span> <span class="nb">.</span>
</code></pre></div></div> <p>Just keep in mind that this requires you to <a href="https://setuptools.pypa.io/en/latest/userguide/quickstart.html">setup the package</a> first.</p> <p>Check the following blogs explaining why using native library:</p> <ul> <li><a href="https://www.bitecode.dev/p/why-not-tell-people-to-simply-use">Why not tell people to “simply” use pyenv, poetry, pipx or anaconda</a></li> <li><a href="https://www.bitecode.dev/p/back-to-basics-with-pip-and-venv">Back to basics with pip and venv</a></li> <li><a href="https://www.b-list.org/weblog/2022/may/13/boring-python-dependencies/">Boring Python: dependency management</a></li> </ul> <h3 id="folder-tree">folder tree</h3> <p>A good folder tree is also very useful managing your project. Here is how I order my stuff.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>projectname/
├── .venv/
├── dist/
│   ├── projectname-1.0.0-py3-none-any.whl
│   └── projectname-1.0.0.tar.gz
├── notebooks/
│   └── notebook.ipynb
├── scripts/
│   └── script.py
├── data/
│   └── data.npy
├── src/
│   └──packagename/
│       ├── __init__.py
│       └── subpackage/
│           ├── __init__.py
│           ├── subpackage.py
├── tests/
│   └── test_subpackage.py
├── docs/
│    └── Makefile
├── .gitignore
├── Makefile
├── Dockerfile
├── requirements/
│   ├── dev-requirements.in
│   ├── dev-requirements.txt
│   ├── requirements.in
│   ├── requirements.txt
│ 
├── pyproject.toml
├── MANIFEST.in
├── tox.ini
├── LICENSE
└── README.md
</code></pre></div></div> <p>You probably already know what they are for by just google their names. Anyway, I would explain the files later when we meet them.</p> <ul> <li><a href="https://gist.github.com/Nivratti/ea81e952e07ffbbf03e6d44a7dbbef8f">directory-structure-deep-learning</a></li> </ul> <h3 id="vscode-pep8-black-flake8-isort-pylance">VScode, pep8, black, flake8, isort, pylance,</h3> <p>Already we have the environment and can code now! Here are some tools that helps you writing better code. I use VScode with the following extensions installed.</p> <ul> <li><code>pylance</code> helps <a href="https://code.visualstudio.com/docs/python/editing">writing codes faster</a></li> <li><code>pep8</code> and <code>flake8</code> helps <a href="https://code.visualstudio.com/docs/python/linting">linting Python codes</a></li> <li><code>black</code> and <code>isort</code> helps <a href="https://code.visualstudio.com/docs/python/formatting">formatting Python codes</a> You can easily installed them in the extensions market of VScode</li> </ul> <h3 id="pytest">pytest</h3> <p>After coding, you should always test your code before publishing. <a href="https://docs.pytest.org/en/stable/index.html#">Pytest</a> facilitate this. Writing test codes is very easy following the official tutorial and after that you only need to run</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>python <span class="nt">-m</span> pytest
</code></pre></div></div> <p>which output clean and clear test results:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">============================= test session starts ==============================
platform darwin -- Python 3.9.13, pytest-8.2.2, pluggy-1.5.0
rootdir: /Users/hous/Github/NeuralHedge
configfile: pyproject.toml
plugins: anyio-4.4.0
collected 4 items                                                              

tests/test_data.py ..                                                    [ 50%]
tests/test_nn.py .                                                       [ 75%]
tests/test_utils.py .                                                    [100%]

=============================== warnings summary ===============================
tests/test_nn.py::test_network
  /Users/hous/Github/NeuralHedge/.venv/lib/python3.9/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
    warnings.warn('Lazy modules are a new feature under heavy development '

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
========================= 4 passed, 1 warning in 3.68s =========================
</span></code></pre></div></div> <ul> <li><a href="https://www.obeythetestinggoat.com/book/preface.html">Obey the Testing Goat!</a></li> </ul> <h3 id="setuptools">setuptools</h3> <p>Now you are ready to publish your codes. I follow the pipelines:</p> <ul> <li><a href="https://packaging.python.org/en/latest/tutorials/packaging-projects/">Packaging Python Projects</a></li> <li><a href="https://setuptools.pypa.io/en/latest/userguide/index.html">User guide: setuptools</a></li> <li><a href="https://realpython.com/pypi-publish-python-package/">How to Publish an Open-Source Python Package to PyP</a></li> </ul> <p>Always check the latest tutorial! <code>pyproject.toml</code> is the new standardized format to describe project metadata declaratively, introduced with <a href="https://peps.python.org/pep-0621/">PEP 621</a>, but many projects are still using the <code>setup.py</code> approach.</p> <h3 id="tox">tox</h3> <p>With <a href="https://tox.wiki/en/latest/index.html">tox</a>, you can even test your codes in different environments. Simply write a configuration of tests and environments.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># tox.in
[tox]
env_list = py38, py39

[testenv]
deps = pytest
command = pytest tests
</code></pre></div></div> <p>Then <code>tox</code> would do everything for you</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>tox
<span class="gp">.pkg: _optional_hooks&gt;</span><span class="w"> </span>python /Users/hous/Github/NeuralHedge/.venv/lib/python3.9/site-packages/pyproject_api/_backend.py True setuptools.build_meta
<span class="gp">.pkg: get_requires_for_build_sdist&gt;</span><span class="w"> </span>python /Users/hous/Github/NeuralHedge/.venv/lib/python3.9/site-packages/pyproject_api/_backend.py True setuptools.build_meta
<span class="gp">.pkg: build_sdist&gt;</span><span class="w"> </span>python /Users/hous/Github/NeuralHedge/.venv/lib/python3.9/site-packages/pyproject_api/_backend.py True setuptools.build_meta
<span class="gp">py38: install_package&gt;</span><span class="w"> </span>python <span class="nt">-I</span> <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--force-reinstall</span> <span class="nt">--no-deps</span> /Users/hous/Github/NeuralHedge/.tox/.tmp/package/4/neuralhedge-0.1.0.tar.gz
<span class="go">py38: OK ✔ in 4.98 seconds
</span><span class="gp">py39: install_package&gt;</span><span class="w"> </span>python <span class="nt">-I</span> <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--force-reinstall</span> <span class="nt">--no-deps</span> /Users/hous/Github/NeuralHedge/.tox/.tmp/package/5/neuralhedge-0.1.0.tar.gz
<span class="go">  py38: OK (4.98 seconds)
  py39: OK (2.94 seconds)
  congratulations :) (8.03 seconds)
</span></code></pre></div></div> <h2 id="reference">Reference</h2> <p>This blog is greatly inspired by</p> <ul> <li><a href="https://www.samueldowling.com/2020/06/08/how-to-set-up-a-python-project-and-development-environment/">How to set up a python project and development environment</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[I’ve recently published a Python package. At least for me, I become more careful and comprehensive to the codes, once decided publishing the project. Immediately, I stand on the opposite an start questioning and criticizing every piece of codes. But I have to say this helps me improve the codes a lot. For example, in machine learning projects, I start thinking how flexible my structure could be adapted to other data and model. Publishing your codes if definitely something you want to try!]]></summary></entry><entry><title type="html">Install Python on Linux without Sudo</title><link href="https://justinhou95.github.io/blog/2024/install-python/" rel="alternate" type="text/html" title="Install Python on Linux without Sudo"/><published>2024-04-20T15:12:00+00:00</published><updated>2024-04-20T15:12:00+00:00</updated><id>https://justinhou95.github.io/blog/2024/install-python</id><content type="html" xml:base="https://justinhou95.github.io/blog/2024/install-python/"><![CDATA[<p>I was running my machine learning project on a Linux server and needed to install Python 3.8 for some packages. I decided to install natively to keep everything under control, see this nice <a href="https://www.bitecode.dev/p/installing-python-the-bare-minimum">blog</a>. Without sudo right, things get a little different from standard installation but I found this nice <a href="https://www.bobbydurrettdba.com/2020/02/11/python-3-8-1-linux-install-without-root/">blog</a> by Bobby Durrett making this super simple.</p> <ul> <li>Download the python from <a href="https://www.python.org">www.python.org</a></li> </ul> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:Downloads] $</span><span class="w"> </span>wget https://www.python.org/ftp/python/3.8.18/Python-3.8.18.tgz
<span class="gp">Python-3.8.18.tgz   100%[===================&gt;</span><span class="o">]</span>  26.07M  <span class="nt">--</span>.-KB/s    <span class="k">in </span>0.1s    
<span class="go">2024-04-20 12:36:05 (230 MB/s) - ‘Python-3.8.18.tgz’ saved [27337549/27337549]
</span><span class="gp">[hous@ada-25:Downloads] $</span><span class="w"> 
</span></code></pre></div></div> <ul> <li>Unzip it</li> <li>Go to the folder</li> </ul> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:Downloads] $</span><span class="w"> </span><span class="nb">tar </span>zxfv Python-3.8.18.tgz 
<span class="go">Python-3.8.18/
Python-3.8.18/Lib/
</span><span class="c">......
</span><span class="go">Python-3.8.18/config.guess
Python-3.8.18/Makefile.pre.in
</span><span class="gp">[hous@ada-25:Downloads] $</span><span class="w"> </span><span class="nb">cd </span>Python-3.8.18/
<span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> 
</span></code></pre></div></div> <ul> <li>Check the absolute path of home directory</li> <li>Make a directory where you want to put your python. I called my <code>.localpython3.8</code> (name it whatever you want) and make it under the home directory.</li> <li>Configured the Python make</li> </ul> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span><span class="nb">pwd</span>
<span class="go">/u/hous/Downloads/Python-3.8.18
</span><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span><span class="nb">mkdir</span> /u/hous/.localpython3.8
<span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span>./configure <span class="nt">--prefix</span><span class="o">=</span>/u/hous/.localpython3.8
<span class="go">checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
</span><span class="c">......
</span><span class="go">creating Modules/Setup.local
creating Makefile
If you want a release build with all stable optimizations active (PGO, etc),
please run ./configure --enable-optimizations
</span><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> 
</span></code></pre></div></div> <ul> <li>Make and make altinstall (see difference between install and altinstall <a href="https://stackoverflow.com/questions/16018463/difference-in-details-between-make-install-and-make-altinstall">here</a>)</li> </ul> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span>make
<span class="go">gcc -c -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall  
</span><span class="c">...
</span><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span>make altinstall
<span class="c">......
</span><span class="go">Successfully installed pip-23.0.1 setuptools-56.0.0
</span><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> 
</span></code></pre></div></div> <p>Now we can already use <code>~/.localpython3.8/bin/python3.8</code> to run python</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:~] $</span><span class="w"> </span>~/.localpython3.8/bin/python3.8
<span class="go">Python 3.8.18 (default, Apr 20 2024, 12:51:15) 
[GCC 13.2.1 20240316 (Red Hat 13.2.1-7)] on linux
Type "help", "copyright", "credits" or "license" for more information.
</span><span class="gp">&gt;</span><span class="o">&gt;&gt;</span> <span class="nb">exit</span><span class="o">()</span>
<span class="gp">[hous@ada-25:~] $</span><span class="w"> 
</span></code></pre></div></div> <p>Perfect! Just to make life easier, we go to <code>~/.localpython3.8/bin/</code> and link <code>python3.8</code> to <code>python</code></p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span><span class="nb">cd</span> ~/.localpython3.8/bin
<span class="gp">[hous@ada-25:bin] $</span><span class="w"> </span><span class="nb">ln</span> <span class="nt">-s</span> python3.8 python
<span class="gp">[hous@ada-25:bin] $</span><span class="w"> </span><span class="nb">ln</span> <span class="nt">-s</span> pip3.8 pip
<span class="gp">[hous@ada-25:bin] $</span><span class="w"> 
</span></code></pre></div></div> <p>Now, we only need to type <code>~/.localpython3.8/bin/python</code>. Still a bit inconvenient, so next we add the folder path to PATH variable (abundant nice tutorial online explaining this). This is not just for convenient but avoid you running the wrong python.</p> <p><code>export PATH=/u/hous/.localpython3.8/bin:$PATH</code> to <code>.bashrc</code> file.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:Python-3.8.18] $</span><span class="w"> </span><span class="nb">cd</span> ~
<span class="gp">[hous@ada-25:~] $</span><span class="w"> </span>vi .bashrc
<span class="gp">#</span><span class="w"> </span>user<span class="s1">'s bashrc
</span><span class="gp">#</span><span class="w">
</span><span class="gp">export PATH=/u/hous/.localpython3.8/bin:$</span><span class="s1">PATH
</span><span class="c">...
</span><span class="gp">[hous@ada-25:~] $</span><span class="w">
</span></code></pre></div></div> <p>After this, we logout and login. Now we can simply type <code>python</code> and check the installation.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">[hous@ada-25:~] $</span><span class="w"> </span>python
<span class="go">Python 3.8.18 (default, Apr 20 2024, 12:51:15) 
[GCC 13.2.1 20240316 (Red Hat 13.2.1-7)] on linux
Type "help", "copyright", "credits" or "license" for more information.
</span><span class="gp">&gt;</span><span class="o">&gt;&gt;</span> <span class="nb">exit</span><span class="o">()</span>
<span class="gp">[hous@ada-25:~] $</span><span class="w"> </span>pip <span class="nt">-V</span>
<span class="go">pip 23.0.1 from /u/hous/.localpython3.8/lib/python3.8/site-packages/pip (python 3.8)
</span><span class="gp">[hous@ada-25:~] $</span><span class="w"> </span>pip list
<span class="go">Package    Version
---------- -------
pip        23.0.1
setuptools 56.0.0

</span><span class="gp">[notice] A new release of pip is available: 23.0.1 -&gt;</span><span class="w"> </span>24.0
<span class="go">[notice] To update, run: pip install --upgrade pip
</span><span class="gp">[hous@ada-25:~] $</span><span class="w"> 
</span></code></pre></div></div> <p>We are done! I usually then create a virtual environment and play with my machine learning codes, see <a href="https://www.b-list.org/weblog/2022/may/13/boring-python-dependencies/">here</a> why venv is a good idea.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://pages.github.nceas.ucsb.edu/NCEAS/Computing/local_install_python_on_a_server.html">https://pages.github.nceas.ucsb.edu/NCEAS/Computing/local_install_python_on_a_server.html</a></li> <li><a href="https://www.bobbydurrettdba.com/2020/02/11/python-3-8-1-linux-install-without-root/">https://www.bobbydurrettdba.com/2020/02/11/python-3-8-1-linux-install-without-root/</a></li> <li><a href="https://thelazylog.com/install-python-as-local-user-on-linux/">https://thelazylog.com/install-python-as-local-user-on-linux/</a></li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[I was running my machine learning project on a Linux server and needed to install Python 3.8 for some packages. I decided to install natively to keep everything under control, see this nice blog. Without sudo right, things get a little different from standard installation but I found this nice blog by Bobby Durrett making this super simple.]]></summary></entry></feed>